
# ğŸ¤– AI-Powered PDF Search Assistant

Tired of manually scanning through long PDFs just to find that one paragraph you need?
This assistant helps you **upload any PDF, ask questions about it, and get summaries** â€” all running *locally* on your machine.

---

## ğŸŒŸ What It Does

- ğŸ“‚ Upload any PDF (books, papers, invoices â€” anything!)
- ğŸ§  Get a smart summary of whatâ€™s inside
- ğŸ’¬ Ask it questions in plain English and get contextual answers
- ğŸ”’ 100% local â€” No API keys or internet needed

---

## ğŸ› ï¸ Built With

- **Python** â€“ backend logic
- **Gradio** â€“ beautiful and simple user interface
- **PyMuPDF** â€“ PDF text extraction
- **SentenceTransformers** â€“ for embeddings
- **ChromaDB** â€“ local vector search engine
- **Ollama** â€“ runs your local LLM (e.g., Mistral, LLaMA3, Gemma)

---

## ğŸ—‚ï¸ Project Structure

```
pdf-search-assistant/
â”œâ”€â”€ src/                  # core logic
â”‚   â”œâ”€â”€ pdf_reader.py     # PDF text extractor
â”‚   â””â”€â”€ qa_pipeline.py    # QA engine: embed, search, answer
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py            # Gradio interface
â”œâ”€â”€ data/                 # where your uploaded PDFs go
â”œâ”€â”€ assets/               # images, icons, etc.
â”œâ”€â”€ requirements.txt      # dependencies
â”œâ”€â”€ .env.example
â””â”€â”€ README.md             # this file
```

---

## ğŸš€ Getting Started

### 1. Clone and install dependencies

```bash
git clone https://github.com/your-username/pdf-search-assistant.git
cd pdf-search-assistant
pip install -r requirements.txt
```

### 2. Start Ollama with your model of choice

```bash
ollama run gemma:2b
```

(You can replace `gemma:2b` with `llama3`, `mistral`, or `yi` if installed)

### 3. Run the app

```bash
python ui/app.py
```

Go to [http://localhost:7860](http://localhost:7860) and start chatting with your PDF!

---

## ğŸ§  How It Works (Behind the Scenes)

1. We read the PDF and split it into text chunks.
2. Those chunks are converted into embeddings using SentenceTransformers.
3. ChromaDB helps us search for the most relevant chunks.
4. Your query + those chunks are sent to a local LLM via Ollama.
5. The model replies with a contextual answer.

Itâ€™s like ChatGPT â€” but with a memory of your document, and all on your machine.

---

## ğŸ¯ Example Use Cases

- ğŸ§¾ Ask questions about academic papers
- ğŸ“š Chat with textbooks or course notes
- ğŸ“„ Extract insights from contracts, reports, resumes
- ğŸ§ª Summarize long technical documentation

---

## ğŸ”® Whatâ€™s Next?

- Multiple PDF support
- Source references in answers
- More powerful summarization
- Streamed LLM responses

---

## ğŸ§‘â€ğŸ’» Author

Made by Vyshnavi Kunapareddy

---

## ğŸªª License

MIT License â€“ Free to use, modify, and share.
